{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Information Retrieval\n",
    "## Instructions\n",
    "1. Students will form teams of three people each and submit a single homework for each team in the format - ID1_ID2_ID3.ipynb\n",
    "2. Groups of three!\n",
    "2. **Do not write your names anywhere.**\n",
    "3. For the code part: \n",
    "> **Write your code only in the mentioned sections. Do not change the code of other sections**. Do not use any imports unless we say so.\n",
    "4. For theoretical questions, if any - write your answer in the markdown cell dedicated to this task, in **English**.\n",
    "\n",
    "\n",
    "#### Deviation from the aforementioned  instructions will lead to reduced grade\n",
    "---\n",
    "\n",
    "\n",
    "## Clarifications\n",
    "1. The same score for the homework will be given to each member of the team.  \n",
    "2. The goal of this homework is to test your understanding of the concepts presented in the lectures. \\\n",
    "Anyhow, we provide here detailed explanations for the code part and if you have problems - ask.\n",
    "3. Questions can be sent to the forum, you are encouraged to ask questions but do so after you have been thinking about your question. \n",
    "4. The length of the empty gaps (where you are supposed to write your code) is a recommendation (the amount of space took us to write the solution) and writing longer code will not harm your grade. We do not expect you to use the programming tricks and hacks we used to make the code shorter.   \n",
    "Having said that, we do encourage you to write good code and keep that in mind - **extreme** cases may be downgraded.  \n",
    "We also encourage to use informative variable names - it is easier for us to check and for you to understand. \n",
    "\n",
    "Since it is the first time we provide this homework please notify us if there is a bug/something is unclear, typo's exc..\n",
    "\n",
    "5. We use Python 3.7 for programming.\n",
    "6. Make sure you have all the packages and functions used in the import section. Most of it is native to Anaconda Python distribution.\n",
    "\n",
    "### Have fun !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Maya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from typing import List,Dict\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import time\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from string import punctuation, ascii_lowercase\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.perf_counter() #Start timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd=os.getcwd()\n",
    "INPUT_FILE_PATH = os.path.join(cwd, \"lyrics.csv\")\n",
    "BOW_PATH = os.path.join(cwd, \"bow.csv\")\n",
    "N_ROWS = 100000\n",
    "CHUNCK_SIZE = 5000\n",
    "tqdm_n_iterations = N_ROWS//CHUNCK_SIZE +1\n",
    "COLS = [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of words /TfIdf model\n",
    "### Implement the following methods:\n",
    "\n",
    "* `preprocess_sentence`: \n",
    "    * Lower case the word\n",
    "    * Ignores it if it's in the stopwords list\n",
    "    * Removes characters which are not in the allowed symbols\n",
    "    * Stems it and appends it to the output sentence\n",
    "    * Discards words with length <= 1\n",
    "    \n",
    "    \n",
    "* `update_counts_and_probabilities`: \n",
    "\n",
    "    * Update self.unigram count (the amount of time each word is in the text)\n",
    "    * Update self.bigram count (two consecutive word occurances)\n",
    "    * Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}   \n",
    "    \n",
    "* `compute_word_document_frequency`:\n",
    "\n",
    "   * For each word count the number of docs it appears in. For example , for the word 'apple' -\n",
    "$$\\sum_{i \\in docs} I(apple \\in doc_i), I := Indicator function$$\n",
    "\n",
    "\n",
    "* `update_inverted_index_with_tf_idf_and_compute_document_norm`:\n",
    "\n",
    "    * Update the inverted index (which currently hold word counts) with tf idf weighing. We will compute tf by dividing with the number of words in each document. \n",
    "    * As we want to calculate the document norm, incrementally update the document norm. pay attention that later we apply sqrt to it to finish the process.\n",
    "\n",
    "#### The result of this code is a bag of words model that already counts for TF-IDF weighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "allowed_symbols = set(l for l in ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence : str) -> List[str]:\n",
    "    '''\n",
    "    this function preprocesses a sentence by tokenizing it, removing stop words, stemming and removing punctuation and non-english words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        a string representing a sentence\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        a list of words\n",
    "    '''\n",
    "    output_sentence = []\n",
    "    for word in word_tokenize(sentence):\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        # Lower case the word\n",
    "        lower_word = word.lower()\n",
    "        \n",
    "        # Ignores it if it's in the stopwords list\n",
    "        if lower_word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        # Removes characters which are not in the allowed symbols\n",
    "        # Create an empty string to store the output\n",
    "        output_string = \"\"\n",
    "        # Iterate over each character in the input string\n",
    "        for letter in lower_word:\n",
    "            # Check if the character is not equal to the specified letter\n",
    "            if letter in allowed_symbols:\n",
    "                # If it is not equal to the specified letter, add it to the output string\n",
    "                output_string += letter\n",
    "\n",
    "        # Stems it and appends it to the output sentence       \n",
    "        stem_word = stemmer.stem(output_string)\n",
    "        \n",
    "        # Discards words with length <= 1\n",
    "        if len(stem_word) <= 1:\n",
    "            continue\n",
    "        else:\n",
    "            output_sentence.append(stem_word)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "    return output_sentence\n",
    "\n",
    "def get_data_chuncks() -> List[str]:\n",
    "    for i ,chunck in enumerate(pd.read_csv(INPUT_FILE_PATH, usecols = COLS, chunksize = CHUNCK_SIZE, nrows = N_ROWS)):\n",
    "        chunck = chunck.values.tolist()\n",
    "        yield [chunck[i][0] for i in range(len(chunck))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdf:\n",
    "    def __init__(self):\n",
    "        #unigram_count is a dictionary that maps a word to the number of times it appears in the corpus\n",
    "        self.unigram_count =  Counter()\n",
    "        #bigram_count is a dictionary that maps a bigram to the number of times it appears in the corpus\n",
    "        self.bigram_count = Counter()\n",
    "        #document_term_frequency is a dictionary that maps a document id to the number of words in the document\n",
    "        self.document_term_frequency = Counter()\n",
    "        #word_document_frequency is a dictionary that maps a word to the number of documents it appears in\n",
    "        self.word_document_frequency = {}\n",
    "        #inverted_index is a dictionary that maps a word to a dictionary that maps a document id to the tf-idf weight of the word in the document\n",
    "        self.inverted_index = {}\n",
    "        #doc_norms is a dictionary that maps a document id to the norm of the document\n",
    "        self.doc_norms = {}\n",
    "        #n_docs is the number of documents in the corpus\n",
    "        self.n_docs = -1\n",
    "        #sentence_preprocesser is a function that preprocesses a sentence\n",
    "        self.sentence_preprocesser = preprocess_sentence\n",
    "        #bow_path is the path to the bag of words\n",
    "        self.bow_path = BOW_PATH\n",
    "\n",
    "    def update_counts_and_probabilities(self, sentence:List[str], document_id:int) -> None:\n",
    "        '''\n",
    "        this function updates the unigram and bigram counts and the inverted index given a sentence and a document id\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sentence : List[str]\n",
    "            a list of words\n",
    "        document_id : int\n",
    "            the id of the document\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        '''\n",
    "        sentence_len = len(sentence)\n",
    "        self.document_term_frequency[document_id] = sentence_len\n",
    "        \n",
    "        for i,word in enumerate(sentence):\n",
    "            ### YOUR CODE HERE\n",
    "            \n",
    "            # Update self.unigram count (the amount of time each word is in the text)\n",
    "            self.unigram_count[word] += 1\n",
    "            \n",
    "            # Update self.bigram count (two consecutive word occurances)\n",
    "            if sentence_len > i + 1:  # if there is another word after the current one, update bigram count of the word\n",
    "                self.bigram_count[sentence[i], sentence[i + 1]] += 1     \n",
    "                            \n",
    "            # Update inverted index: a dictionary with words as keys and the values is a dictionary - {'DocID' : word_count}            \n",
    "            count_dic = self.inverted_index.get(word, Counter())  # take the dictionary that holds for each files how many times the word appears \n",
    "            count_dic[document_id] += 1  # update the dictionary for the current file\n",
    "            self.inverted_index[word] = count_dic  # update the word's dictionary\n",
    "            \n",
    "            ### END YOUR CODE\n",
    "            \n",
    "\n",
    "    def fit(self) -> None:\n",
    "        for chunck in tqdm(get_data_chuncks(), total = tqdm_n_iterations):\n",
    "            for sentence in chunck: #sentence is a song (string)\n",
    "                self.n_docs += 1 \n",
    "                if not isinstance(sentence, str):\n",
    "                    continue\n",
    "                sentence = self.sentence_preprocesser(sentence)\n",
    "                if sentence:\n",
    "                    self.update_counts_and_probabilities(sentence,self.n_docs)\n",
    "        self.save_bow() # bow is 'bag of words'\n",
    "        self.compute_word_document_frequency()\n",
    "        self.update_inverted_index_with_tf_idf_and_compute_document_norm()\n",
    "             \n",
    "    def compute_word_document_frequency(self):\n",
    "        '''\n",
    "        this function computes the word document frequency for each word in the inverted index. e.g. if a word appears in 10 documents, the word document frequency is 10.\n",
    "        '''\n",
    "        for word in self.inverted_index.keys():\n",
    "            ### YOUR CODE HERE\n",
    "            self.word_document_frequency[word] = len(self.inverted_index[word].keys())\n",
    "                            \n",
    "            ### END YOUR CODE\n",
    "    def update_inverted_index_with_tf_idf_and_compute_document_norm(self):\n",
    "        '''\n",
    "        This function updates the inverted index with tf-idf weighting and computes the document norm for each document.\n",
    "        '''\n",
    "        for word in self.inverted_index.keys():  # go over the words in the inverted index dic\n",
    "            df = self.word_document_frequency[word]  # Determine the count of files that contain the given word\n",
    "            idf = np.log10((self.n_docs + 1) / float(df))  # calculate the Inverse Document Frequency\n",
    "            word_quant = self.inverted_index[word]  # Retrieve the dictionary that indicates the frequency of the given word in each file\n",
    "            for doc_id in word_quant.keys():  # go over the files that the word appears in\n",
    "                tf = word_quant[doc_id] / self.document_term_frequency[doc_id]  # calculate tf \n",
    "                word_quant[doc_id] = tf * idf  # update the dictionary, wij instead of word count in the file\n",
    "                self.doc_norms[doc_id] = self.doc_norms.get(doc_id, 0) + (word_quant[doc_id]) ** 2  # calculate file norm\n",
    "            self.inverted_index[word] = word_quant  # update word's dic\n",
    "                      \n",
    "        ### END YOUR CODE\n",
    "        for doc in self.doc_norms.keys():\n",
    "            self.doc_norms[doc] = np.sqrt(self.doc_norms[doc]) \n",
    "            \n",
    "    def save_bow(self):\n",
    "        pd.DataFrame([self.inverted_index]).T.to_csv(self.bow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [18:47<00:56, 56.40s/it]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfIdf()\n",
    "tf_idf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Bag of words model:\n",
    "\n",
    "1. What is the computational complexity of this model, as a function of the number of docs in the corpus?\n",
    "2. How can we make this code better in terms running time (parallelization or other topics you find)? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### YOUR SOLUTION HERE\n",
    "1. \n",
    "N = number of documents in the corpus\n",
    "V = size of the vocabulary\n",
    "\n",
    "The functions we have:\n",
    "\n",
    "C(init) = O(1) - set the variabels\n",
    "C(get_data_chuncks) = O(N) - runs through all documents in the corpus\n",
    "C(preprocess_sentence) = O(words amount in corpus/N) = O(1) - for simplicity - runs over 1 file in the corpus\n",
    "C(update_counts_and_probabilities) = O(words amount in corpus/N) = O(1) - for simplicity - runs over 1 file in the corpus\n",
    "C(fit) = O(N) - runs through all forms in the corpus\n",
    "C(compute_word_document_frequency) = O(N) - runs over the entire corpus\n",
    "C(update_inverted_index_with_tf_idf_and_compute_document_norm) = O(NV)\n",
    "\n",
    "the total computational complexity is O(NV)\n",
    "\n",
    "2. a. Parallelization- we can reduce the total running time of the code by using different computers to run the model- Distributed Computing. in our code the data is already separated into data chunks in the fit function, but the computer runs over each data chunk separately, one by one. Therefore, splitting the data chunks into more computers will enable us to run the code in parallel and unite it all together to one set at the end of the process to present a better runtime.\n",
    "\n",
    "b. Hardware upgrades: Upgrading hardware can also be an effective way to reduce the running time. This can include upgrading the CPU, adding more memory, or using solid-state drives (SSDs) instead of traditional hard disk drives (HDDs). By doing so, it's possible to improve the performance of the system and reduce the overall running time. We noticed that using a modern computer with a better processor (Mac) resulted in significantly faster running.\n",
    "\n",
    "c. Caching: Caching can also help to reduce the running time by storing frequently accessed data in memory or on disk. This can include storing the results of common queries or precomputed features of the documents, which can be quickly accessed and used to speed up subsequent queries.\n",
    "\n",
    "### END YOUR SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DocumentRetriever\n",
    "Not this retriever &#8595;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![dsafdsafsdafdsf](https://cdn3-www.dogtime.com/assets/uploads/2019/10/golden-cocker-retriever-mixed-dog-breed-pictures-cover-1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the following methods:\n",
    "\n",
    "`reduce_query_to_counts`: given a list of words returns a counter object with words as keys and counts as values.\n",
    "\n",
    "`rank`: given a query and relevant documents calculate the similarity (cosine or inner product simialrity) between each document and the query.   \n",
    "Make sure to transform the query word counts to tf idf as well. \n",
    "\n",
    "`sort_and_retrieve_k_best`: returns the top k documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRetriever:\n",
    "    def __init__(self, tf_idf):\n",
    "        #sentence_preprocesser is a function that preprocesses a sentence\n",
    "        self.sentence_preprocesser = preprocess_sentence  \n",
    "        #vocab is a set of all the words in the corpus\n",
    "        self.vocab = set(tf_idf.unigram_count.keys())\n",
    "        #n_docs is the number of documents in the corpus\n",
    "        self.n_docs = tf_idf.n_docs\n",
    "        #inverted_index is a dictionary that maps a word to a dictionary that maps a document id to the tf-idf weight of the word in the document\n",
    "        self.inverted_index = tf_idf.inverted_index\n",
    "        #word_document_frequency is a dictionary that maps a word to the number of documents it appears in\n",
    "        self.word_document_frequency = tf_idf.word_document_frequency\n",
    "        #doc_norms is a dictionary that maps a document id to the norm of the document\n",
    "        self.doc_norms = tf_idf.doc_norms\n",
    "        \n",
    "    def rank(self, query:Dict[str,int], documents:Dict[str,Counter], metric:str) -> Dict[str, float]:\n",
    "        '''\n",
    "        this function ranks the documents according to the query and the chosen similarity metric\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query : Dict[str,int]\n",
    "            a dictionary of words and their counts in the query\n",
    "        documents : Dict[str,Counter]\n",
    "            a dictionary of documents that contain the query words and their tf-idf weights\n",
    "        metric : str\n",
    "            the similarity metric to use. can be 'cosine' or 'dot'\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "            a dictionary of documents and their similarity to the query\n",
    "        '''\n",
    "        result = {} # key: DocID , value : float , simmilarity to query\n",
    "        query_len = np.sum(np.array(list(query.values())))\n",
    "        ### YOUR CODE HERE        \n",
    "        for word in query.keys():  # go over the words in the query\n",
    "            q_df = self.word_document_frequency[word]  # save the number of files that the word appears in them\n",
    "            q_idf = np.log10((self.n_docs + 1)/float(q_df))  # calculate idf according to the formula\n",
    "            q_tf = query[word]/query_len  # calculate tf according to the formula\n",
    "            query[word] = q_tf * q_idf # calculate tfidf according to the formula\n",
    "        # inner product - creating the numerator\n",
    "        for term in documents.keys():  # go over the words in the query\n",
    "            for doc_id in documents[term].keys():  # go over the docs the word appears in\n",
    "                wij = documents[term][doc_id]  # take the tfidf value\n",
    "                result[doc_id] = result.get(doc_id, 0) + wij * query[term]  # calculate the similarity of each doc to the query\n",
    "                            \n",
    "        ### END YOUR CODE\n",
    "        if metric == 'cosine':\n",
    "            ### YOUR CODE HERE\n",
    "            q_norm = np.sqrt(np.sum(np.square(list(query.values())))) \n",
    "            for doc_id in result.keys():\n",
    "                result[doc_id] = result[doc_id] / (q_norm * self.doc_norms[doc_id])  # normelaized the similarity\n",
    "\n",
    "            ### END YOUR CODE\n",
    "        return result\n",
    "        \n",
    "           \n",
    "    def sort_and_retrieve_k_best(self, scores:Dict[str, float], k:int)-> List[str]:\n",
    "        '''\n",
    "        this function sorts the documents according to their similarity to the query and returns the k best documents\n",
    "        '''\n",
    "        ### YOUR CODE HERE \n",
    "\n",
    "        return sorted(scores, key=scores.get, reverse=True)[0:k]\n",
    "    \n",
    "        ### END YOUR CODE\n",
    "\n",
    "    \n",
    "    def reduce_query_to_counts(self, query:List)->  Counter:\n",
    "        '''\n",
    "        this function reduces the query to a dictionary of words and their counts\n",
    "        '''\n",
    "        ### YOUR CODE HERE\n",
    "        return Counter(query)\n",
    "        ### END YOUR CODE \n",
    "        \n",
    "        \n",
    "    def get_top_k_documents(self, query:str, metric:str , k=5) -> List[str]:\n",
    "        query = self.sentence_preprocesser(query)\n",
    "        query = [word for word in query if word in self.vocab] # filter nan \n",
    "        query_bow = self.reduce_query_to_counts(query)\n",
    "        relavant_documents = {word : self.inverted_index.get(word) for word in query}\n",
    "        ducuments_with_similarity = self.rank(query_bow,relavant_documents, metric)\n",
    "        return self.sort_and_retrieve_k_best(ducuments_with_similarity,k)\n",
    "        \n",
    "dr = DocumentRetriever(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "query = \"Better stop dreaming of the quiet life, 'cause it's the one we'll never know And quit running for that runaway bus 'cause those rosy days are few And stop apologizing for the things you've never done 'Cause time is short and life is cruel but it's up to us to change This town called malice\"\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/KT6ZtUbVw1M?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50221, 50233, 28634, 38867, 27766]\n",
      "[26464, 38867, 7344, 43026, 43185]\n"
     ]
    }
   ],
   "source": [
    "cosine_top_k = dr.get_top_k_documents(query, 'cosine')\n",
    "print(cosine_top_k)\n",
    "inner_product_top_k = dr.get_top_k_documents(query, 'inner_product')\n",
    "print(inner_product_top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "song #0 \n",
      "A Town Called Malice. Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #1 \n",
      "A Town Called Malice. Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #2 \n",
      "Better stop dreaming of the quiet life. 'Cos it's the one we'll never know. And quit running for that runaway bus. 'Cos those rosey days are few. And stop apologising for the things you've never done,. 'Cos time is short and life is cruel. Well it's up to us to change. This town called malice uh uh yeah. Rows and rows of disused milk floats. stand dying in the dairy yard. And a hundred lonely housewives clutch empty milk. bottles to their hearts. Hanging out their old love letters on the line to dry. It's enough to make you stop believing when tears come. fast and furious. In a town called malice uh uh yeah. Struggle after struggle. Year after year. The atmosphere's a fine blend of ice. I'm almost stone cold dead. In a town called malice uh uh yeah. A whole street's belief in Sunday's roast beef. gets dashed against the Co-op. To either cut down on beer or the kids new gear. It's a big decision in a town called malice uh uh yeah. The ghost of a steam train. Echoes down my track. It's at the moment bound for nowhere. Just going round and round. Playground kids and creaking swings. Lost laughter in the breeze. I could go on for hours and I probably will. But I'd sooner put some joy back. In this town called malice yeah yeah \n",
      "##################################################\n",
      "##################################################\n",
      "song #3 \n",
      "Rosie whatcha doing in this low class joint,. dancing in the dark all day. You used to be the darling of your high school scene. Now you put your love on display. Sweaty hands hand you up a dollar bill.. Hungry eyes never seem to get their fill. I used to watch you walking down the hall.. Rosie do you see me when you hear them call your name.. Rosie, Rosie I wanna take you away.. Rosie, Rosie I'm gonna make you mine someday.. Rosie I went with you for that rose tattoo. You promised no one else would see. I used to wait and drive you home from dancing school.. Remember when you danced just for me. Our love was deeper than the night was long. But things just didn't work out like our favorite song. I used to watch you walking down the hall. Rosie do you see me when you hear them call your name. Rosie, Rosie, I wanna take you away.. Rosie, Rosie, I'm gonna make you mine someday. Rosie, Rosie. Do you remember our love was deeper than the night was long. But things just didn't work out like our favorite song. I used to watch you walking down the hall. Rosie do you see me when you hear them call your name--ROSIE. Rosie, Rosie, I wanna take you away.. Rosie, Rosie, I'm gonna make you mine someday. Rosie, Rosie. \n",
      "##################################################\n",
      "##################################################\n",
      "song #4 \n",
      "(Rosy Won't You Please Come Home). Rosy, won't you please come home,. Mama don't know where you've been.. Rosy won't you please come home,. Your room's clean and no one's in it.. Oh, my Rosy, how I miss you,. You are all the world to me.. Take a look and see if you like it,. If you like it, please, come back.. Rosy, won't you please come home,. Since you joined the upper classes. You don't know us anymore,. Live and let your trouble pass.. Rosy, Rosy, where are you, tell me,. If you don't want to come back.. I will sacrifice all I have. To have a happy home once more.. Rosy, won't you please come home,. Two long years have passed away.. Since you tried to change your life. Christmas wasn't quite the same.. Rosy, Rosy, got any answers ?. You are miles across the sea. And I'll bake a cake if you tell me. You are on the first plane home.. Rosy, won't you please come home,. Mama don't know where you've been.. Rosy won't you please come home,. Your room's clean and no one's in it.. Oh, my Rosy, how I miss you,. You are all the world to me.. Take a look and see if you like it,. If you like it, please, come home.. \n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "for index, song in enumerate(pd.read_csv(INPUT_FILE_PATH,usecols = [0]).iloc[cosine_top_k]['Lyric']):\n",
    "    sep = \"#\"*50\n",
    "    print(F\"{sep}\\nsong #{index} \\n{song} \\n{sep}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 term statistics:\n",
    "Use \"tf_idf\" object that we created earlier and answer the following questions:\n",
    "\n",
    "1. How many unique words we have?\n",
    "2. How many potential word bigrams we have? How many actual word bigrams we have? How do you explain this difference?\n",
    "3. What is the storage size of the input file \"lyrics.csv\"? What is the output file (bow.csv) size? how do you explain this difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100220 unique words\n",
      "The potential bigram words amount is 10043948180\n",
      "There are actualy 2484530 bigram words\n",
      "Input file size is 168.274338 MB\n",
      "Output file size is 76.095002 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n### Your verbal solution here\\n\\nThe input file takes up 168.27 MB of storage space, whereas the output file only requires 76.09 MB. \\nThis difference in size is because the output file is based on a bag-of-words approach, \\nwhich only includes the unique words from the inverted index. \\nAs a result, the output file requires less storage space compared to the input file.\\n\\n### End your verbal solution here\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. \n",
    "### YOUR SOLUTION HERE\n",
    "u_w_amount = len(tf_idf.unigram_count.keys())\n",
    "print(f'There are {u_w_amount} unique words')\n",
    "### END YOUR SOLUTION\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "There are 100220 unique words in the data set.\n",
    "### End your verbal solution here\n",
    "\"\"\"\n",
    "\n",
    "# 2.\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "potential_bigrams = u_w_amount*(u_w_amount-1)\n",
    "bbigrams_amount = len(tf_idf.bigram_count.keys())\n",
    "print(f'The potential bigram words amount is {potential_bigrams}')\n",
    "print(f'There are actualy {bbigrams_amount} bigram words')\n",
    "\n",
    "### END YOUR SOLUTION\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "\n",
    "Out of all the possible combinations of words in the data, there are 10043948180 potential word bigrams. \n",
    "However, the actual number of bigrams we have is only 2484530. \n",
    "The reason for this difference is that the potential bigrams include all possible pairings of words, \n",
    "while the actual bigrams only consider pairs of words where one word comes before the other. \n",
    "Therefore, it's not surprising that the number of potential bigrams is greater than the number of actual bigrams.\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\"\n",
    "\n",
    "# 3.\n",
    "### YOUR SOLUTION HERE\n",
    "\n",
    "input_file_info = os.stat(INPUT_FILE_PATH)\n",
    "print(\"Input file size is\",input_file_info.st_size*0.000001,\"MB\")\n",
    "\n",
    "output_file_info = os.stat(BOW_PATH)\n",
    "print(\"Output file size is\",output_file_info.st_size*0.000001,\"MB\")\n",
    "\n",
    "### END YOUR SOLUTION\n",
    "\n",
    "\"\"\"\n",
    "### Your verbal solution here\n",
    "\n",
    "The input file takes up 168.27 MB of storage space, whereas the output file only requires 76.09 MB. \n",
    "This difference in size is because the output file is based on a bag-of-words approach, \n",
    "which only includes the unique words from the inverted index. \n",
    "As a result, the output file requires less storage space compared to the input file.\n",
    "\n",
    "### End your verbal solution here\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 NgramSpellingCorrector\n",
    "Now we will implement a Ngarm (character Ngrams) spelling corrector. That is, we have an out of vocabulary word (v) and we want to retrieve the most similar words (in our vocabulary) to this word.\n",
    "we will model the similarity of two words by-\n",
    "\n",
    "$$JaccardIndex =  \\frac{|X \\cap Y|}{|X \\cup Y|}$$\n",
    "\n",
    "Where v is an out of vocabulary word (typo or spelling mistake), w is in a vocabulary word, X is the ngram set of v and Y is the ngram set of w.\n",
    "For example, if n == 3, the set of ngrams for word \"banana\" is set(\"ban\",\"ana\",\"nan\",\"ana\") = {\"ban\",\"ana\",\"nan\"}\n",
    "\n",
    "In order to do it efficently, we will first construct an index from the possible Ngrams we have seen in our corpus to the words that those Ngrams appear in, in order prevent comparing v to all of the words in our corpus.\n",
    "Then, we will implement a function that computes this similarity.\n",
    "\n",
    "* Make sure you compute the JaccardIndex efficently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor example - get_bigrams is a generator, which is an object we can loop on:\\nfor ngram in get_bigrams(word):\\n    DO SOMETHING\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_bigrams(word):\n",
    "    for ngram in nltk.ngrams(word, 2):\n",
    "        yield \"\".join(list(ngram))\n",
    "    \n",
    "\"\"\"\n",
    "for example - get_bigrams is a generator, which is an object we can loop on:\n",
    "for ngram in get_bigrams(word):\n",
    "    DO SOMETHING\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramSpellingCorrector:\n",
    "    def __init__(self, unigram_counts: Counter, get_n_gram: callable):\n",
    "        #unigram_counts is a dictionary that maps a word to its count in the corpus\n",
    "        self.unigram_counts = unigram_counts\n",
    "        #ngram_index is a dictionary that maps an ngram to a set of words that contain it\n",
    "        self.ngram_index = {}\n",
    "        #get_n_grams is a function that returns the ngrams of a word\n",
    "        self.get_n_grams = get_n_gram\n",
    "    \n",
    "    def build_index(self) -> None:\n",
    "        '''\n",
    "        this function builds an index of ngrams to words that contain them\n",
    "        the structure of the index is a dictionary of ngrams to sets of words\n",
    "        '''\n",
    "        ### YOUR CODE HERE\n",
    "        for word in self.unigram_counts: # goes over the words\n",
    "            for ngram in set(list(self.get_n_grams(word))):  # split the word into bigrams \n",
    "                if ngram in self.ngram_index:  # if the ngram is in the ngram index dictionary add the word to his list\n",
    "                    self.ngram_index[ngram].append(word)\n",
    "                else:  # if the ngram not in the ngram index dictionary add him for the first time \n",
    "                    self.ngram_index[ngram] = [word]\n",
    "        return\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def get_top_k_words(self,word:str,k=5) -> (List[str], Dict[str, float]):\n",
    "        '''\n",
    "        this function builds an index of ngrams to words that contain them\n",
    "        the structure of the index is a dictionary of ngrams to sets of words\n",
    "        '''\n",
    "        ### YOUR CODE HERE\n",
    "        candidate_words = []  # a list to hold all the words that has mutual bigrams with the given word\n",
    "        for bgram in self.get_n_grams(word):  # go over the bigrams of the word\n",
    "            candidate_words += self.ngram_index[bgram]  # add all the words with mutual bigram to the candidate words\n",
    "        # turn the list into a counter that counts the number of times each word appear with a mutual bigram\n",
    "        \n",
    "        word_counter = Counter(candidate_words)                \n",
    "        jaccard_index =  {}  \n",
    "        v_len = len(list(self.get_n_grams(word)))\n",
    "        for word in word_counter:  # go over the counter dictionary\n",
    "            w_len = len(list(self.get_n_grams(word)))  # number of bigrams in the current word\n",
    "            jaccard_index[word] = (word_counter[word] / (v_len + w_len - word_counter[word]))  # calculate jaccard index\n",
    "        top_k_words = sorted(jaccard_index, key=jaccard_index.get, reverse=True)[:k]  # sort by jaccard_index and get the first k similiar words\n",
    "        return top_k_words, jaccard_index\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "class BigramSpellingCorrector(NgramSpellingCorrector):\n",
    "    def __init__(self, unigram_counts: Counter):\n",
    "        super().__init__(unigram_counts, get_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Understanding Question:\n",
    "\n",
    "In the \"get top k words\" function, does each candidate word have repeat ones or there is a channce that the same word will be iterated more than once? Explain your answer.\n",
    "\n",
    "Answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_vocab_word = 'acress'\n",
    "bigram_spelling_corrector = BigramSpellingCorrector(tf_idf.unigram_count)\n",
    "bigram_spelling_corrector.build_index()\n",
    "candidate_words, scores = bigram_spelling_corrector.get_top_k_words(out_of_vocab_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cress', 'ress', 'actress', 'cresson', 'recress']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccared score for the word cress is 0.8\n",
      "The jaccared score for the word ress is 0.6\n",
      "The jaccared score for the word actress is 0.57\n",
      "The jaccared score for the word cresson is 0.57\n",
      "The jaccared score for the word recress is 0.57\n"
     ]
    }
   ],
   "source": [
    "for word in candidate_words:\n",
    "    print(f'The jaccared score for the word {word} is {round(scores[word],2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End - You did it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time it took to run the entire code is: 29.862189945000015 minuts\n"
     ]
    }
   ],
   "source": [
    "#The time it took to run the entire code\n",
    "toc = time.perf_counter()\n",
    "print(f\"The time it took to run the entire code is: {(toc - tic)/60} minuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "0162daef2ac4f91d71dc659d7366b1318efa6dce3a9605ecac659f5b282e8a3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
